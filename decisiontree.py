# -*- coding: utf-8 -*-
"""DecisionTree.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1NXs1XbyAJnrBlRLwYttWMtYIT9Lq7UFy
"""

# DECISION TREE IMPLEMENTATION- ENTROPY AND INFO GAIN

import pandas as pd  
df_tennis =pd.read_csv("/content/tennis.csv") 
df_tennis

def entropy(probs): 
    import math 
    return sum( [-prob*math.log(prob, 2) for prob in probs] )

def entropy_of_list(a_list): 
    from collections import Counter     
    cnt = Counter(x for x in a_list)
    print("No and Yes Classes:",a_list.name,cnt)     
    num_instances = len(a_list)*1.0     
    probs = [x / num_instances for x in cnt.values()]     
    return entropy(probs) # Call Entropy: 
total_entropy = entropy_of_list(df_tennis['PlayTennis']) 
print("Entropy of given PlayTennis Data Set:",total_entropy)

def information_gain(df, split_attribute_name, target_attribute_name, trace=0): 
    df_split = df.groupby(split_attribute_name) 
    for name,group in df_split:         
        print(name)         
        print(group)    
    nobs = len(df.index) * 1.0
    df_agg_ent = df_split.agg({target_attribute_name : [entropy_of_list, lambda x: len(x)/nobs] })[target_attribute_name] 
    df_agg_ent.columns = ['Entropy', 'PropObservations'] 
    new_entropy = sum(df_agg_ent['Entropy'] * df_agg_ent['PropObservations']) #entropy of the split attribute i.e. Outlook, temp etc
    old_entropy = entropy_of_list(df[target_attribute_name]) #old_entropy is entropy of dataset
    return old_entropy - new_entropy

print('Info-gain for Outlook is :'+str( information_gain(df_tennis, 'Outlook', 'PlayTennis')),"\n") 
print('\n Info-gain for Humidity is: ' + str( information_gain(df_tennis, 'Humidity', 'PlayTennis')),"\n") 
print('\n Info-gain for Wind is:' + str( information_gain(df_tennis, 'Wind', 'PlayTennis')),"\n") 
print('\n Info-gain for Temperature is:' + str( information_gain(df_tennis , 'Temperature','PlayTennis')),"\n")

def id3(df, target_attribute_name, attribute_names, default_class=None):
    from collections import Counter     
    cnt = Counter(x for x in df[target_attribute_name])
    if len(cnt) == 1:         
        return next(iter(cnt)) 
    elif df.empty or (not attribute_names): 
             return default_class 
    else:
        gainz = [information_gain(df, attr, target_attribute_name) for attr in attribute_names] 
        index_of_max = gainz.index(max(gainz)) 
        best_attr = attribute_names[index_of_max] 
        tree = {best_attr:{}}
        remaining_attribute_names = [i for i in attribute_names if i != best_attr]
        for attr_val, data_subset in df.groupby(best_attr): 
            subtree = id3(data_subset,
                          target_attribute_name,                         
                          remaining_attribute_names,                         
                          default_class)
            tree[best_attr][attr_val] = subtree 
        return tree

attribute_names = list(df_tennis.columns)
print("List of Attributes:", attribute_names) 
attribute_names.remove('PlayTennis') 
print("Predicting Attributes:", attribute_names)

from pprint import pprint 
mytree = id3(df_tennis,'PlayTennis',attribute_names)
print("\n\nThe Resultant Decision Tree is :\n") 
pprint(mytree)

"""This is extra codes...not for lab"""

import pydot

def draw(parent_name, child_name):
    edge = pydot.Edge(parent_name, child_name)
    graph.add_edge(edge)

def visit(node, parent=None):
    for k,v in node.items():
        if isinstance(v, dict):
            # We start with the root node whose parent is None
            # we don't want to graph the None node
            if parent:
                draw(parent, str(k))
            visit(v, k)
        else:
            draw(parent, str(k))
            # drawing the label using a distinct name
            draw(str(k), v)

graph = pydot.Dot(graph_type='graph')
visit(mytree)
graph.write_png('example1_graph.png')

from PIL import Image
filename="example1_graph.png"
Image.open(filename)

def predict(inst,trees):
    for nodes in trees.keys():
        value = inst[nodes]
        trees = trees[nodes][value]
        prediction = 0
            
        if type(trees) is dict:
            prediction = predict(inst, trees)
        else:
            prediction = trees
            break;                            
        
    return prediction

inst = {'Outlook':'rainy','Temperature':'hot', 'Humidity':'high', 'Wind':False} 
preds = predict(inst, mytree)
preds

"""ANOTHER METHOD "USING SCIKIT LEARN"

#modifying string values to int for fitting

dataset =pd.read_csv("/content/tennis.csv") 

dataset.Outlook[dataset.Outlook=='sunny'] = 1
dataset.Outlook[dataset.Outlook=='rainy'] = 2
dataset.Outlook[dataset.Outlook=='overcast'] = 3

dataset.Temperature[dataset.Temperature=='hot'] = 1
dataset.Temperature[dataset.Temperature=='mild'] = 2
dataset.Temperature[dataset.Temperature=='cool'] = 3


dataset.Humidity[dataset.Humidity=='high'] = 1
dataset.Humidity[dataset.Humidity=='normal'] = 2

dataset.Wind[dataset.Wind=='False'] = 1
dataset.Wind[dataset.Wind=='True'] = 2

from sklearn import tree
from sklearn import model_selection

array = dataset.values
X=array[:, 0:4]
Y=array[:, 4]
validation_size=0.30
seed=7
X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size = validation_size, random_state=seed)


clf = tree.DecisionTreeClassifier(criterion='entropy')
clf.fit(X_train, Y_train)
predictions = clf.predict(X_validation)

print(X_validation)
print("Predictions = " , predictions)
print("Actual values = " , Y_validation)
"""

