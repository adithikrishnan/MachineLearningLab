# -*- coding: utf-8 -*-
"""ANN2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1a0_mAEMu63Hg1QL9jlaNGPcZln9q16Tx
"""

import numpy as np

X=np.array(([2,9], [1,5], [3,6]), dtype=float)
denom = np.amax(X, axis=0) #prints single element with max(x1) and max(x2)
X=X/denom
Y=np.array(([92], [86], [89]), dtype=float)
Y=Y/100

print(X)
print(Y)

#Sigmoid Activation Function
def sigmoid(x):
  return 1/1+np.exp(-x)

#Derivative of Sigmoid
def derivatives_sigmoid(x):
  return x * (1-x)

iterations = 50000
learning_rate = 0.1
inputlayer_neurons = 2
hiddenlayer_neurons = 3
output_neurons = 1

weight_hidden=np.random.uniform(size=(inputlayer_neurons, hiddenlayer_neurons))
bias_hidden = np.random.uniform(size=(1, hiddenlayer_neurons))
weight_output=np.random.uniform(size=(hiddenlayer_neurons, output_neurons))
bias_output = np.random.uniform(size=(1, output_neurons))

print(weight_hidden , "\n",  bias_hidden, "\n", weight_output, "\n", bias_output)

#FORWARD PASS
for i in range(iterations):
  hidden = np.dot(X, weight_hidden)
  hinp = hidden + bias_hidden
  hlayer_act = sigmoid(hinp)
  #print(hidden, "\n", hinp, "\n", hlayer_act)
  output_y = np.dot(hlayer_act, weight_output) + bias_output
  output_act = sigmoid(output_y)
  #print(output_act)
  
  #BACKPROPAGATION
  error_output = Y - output_act
  output_grad = derivatives_sigmoid(output_act)
  derivative_output = error_output * output_grad

  error_hidden = derivative_output.dot(weight_output.T)
  hidden_grad = derivatives_sigmoid(hlayer_act)
  derivative_hidden = error_hidden * hidden_grad
  
  #UPDATING WEIGHTS
  weight_output += hlayer_act.T.dot(derivative_output) * learning_rate
  weight_hidden += X.T.dot(derivative_hidden) * learning_rate

print("X is :\n", X)
print("Y is :\n", Y)
print("Predicted value of Y is :\n", output_act)

